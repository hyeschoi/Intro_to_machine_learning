\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}CS189: Introduction to Machine Learning}{1}{section.1}}
\newlabel{cs189-introduction-to-machine-learning}{{1}{1}{CS189: Introduction to Machine Learning}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Problem 1: Ridge Regression}{1}{subsection.1.1}}
\newlabel{problem-1-ridge-regression}{{1.1}{1}{Problem 1: Ridge Regression}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}1a)}{1}{subsubsection.1.1.1}}
\newlabel{a}{{1.1.1}{1}{1a)}{subsubsection.1.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}1b)}{1}{subsubsection.1.1.2}}
\newlabel{b}{{1.1.2}{1}{1b)}{subsubsection.1.1.2}{}}
\gdef \LT@i {\LT@entry 
    {1}{41.69456pt}\LT@entry 
    {2}{93.77805pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Problem 2 : Logistic Regression}{5}{subsection.1.2}}
\newlabel{problem-2-logistic-regression}{{1.2}{5}{Problem 2 : Logistic Regression}{subsection.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Problem 3: Spam classification using Logistic Regression}{6}{subsection.1.3}}
\newlabel{problem-3-spam-classification-using-logistic-regression}{{1.3}{6}{Problem 3: Spam classification using Logistic Regression}{subsection.1.3}{}}
\newlabel{preprocessing}{{1.3}{7}{Preprocessing}{section*.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Preprocessing}{7}{section*.1}}
\newlabel{batch-gradient-descent}{{1.3}{7}{Batch Gradient Descent}{section*.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Batch Gradient Descent}{7}{section*.2}}
\newlabel{stochastic-gradient-descent}{{1.3}{121}{Stochastic Gradient Descent}{section*.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Stochastic Gradient Descent}{121}{section*.3}}
\newlabel{stochastic-gradient-descent-vs-batch-gradient-descent}{{1.3}{291}{Stochastic gradient descent VS Batch gradient descent}{section*.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Stochastic gradient descent VS Batch gradient descent}{291}{section*.4}}
\newlabel{stochastic-gradient-descent-with-learning-rate-decreasing-over-iterations}{{1.3}{291}{Stochastic gradient descent with learning rate decreasing over iterations}{section*.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Stochastic gradient descent with learning rate decreasing over iterations}{291}{section*.5}}
\newlabel{is-this-strategy-better-than-having-a-constant-epsilon}{{1.3}{461}{Is this strategy better than having a constant \(\epsilon \)?}{section*.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Is this strategy better than having a constant \(\epsilon \)?}{461}{section*.6}}
\newlabel{kernel-logistic-regression-with-a-polynomial-kernel-of-degree-2}{{1.3}{461}{kernel logistic regression with a polynomial kernel of degree 2}{section*.7}{}}
\@writefile{toc}{\contentsline {paragraph}{kernel logistic regression with a polynomial kernel of degree 2}{461}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{Repeat the same experiment with the linear kernel}{1576}{section*.8}}
\newlabel{fold-cross-validation}{{1.3}{1576}{10-fold Cross Validation}{section*.9}{}}
\@writefile{toc}{\contentsline {paragraph}{10-fold Cross Validation}{1576}{section*.9}}
\newlabel{validation-risk}{{1.3}{2133}{Validation risk}{section*.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Validation risk}{2133}{section*.10}}
\newlabel{does-the-quadratic-kernel-overfit-the-data}{{1.3}{2690}{Does the quadratic kernel overfit the data?}{section*.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Does the quadratic kernel overfit the data?}{2690}{section*.11}}
\newlabel{for-each-kernel-should-you-decrease-or-increase-to-try-to-improve-performance}{{1.3}{2690}{For each kernel, should you decrease or increase \$\lambda \$ to try to improve performance?}{section*.12}{}}
\@writefile{toc}{\contentsline {paragraph}{For each kernel, should you decrease or increase \$\lambda \$ to try to improve performance?}{2690}{section*.12}}
\newlabel{kaggle}{{1.3}{2691}{Kaggle}{section*.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Kaggle}{2691}{section*.13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Problem 4: Revisiting Logistic Regression}{3062}{subsubsection.1.3.1}}
\newlabel{problem-4-revisiting-logistic-regression}{{1.3.1}{3062}{Problem 4: Revisiting Logistic Regression}{subsubsection.1.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Problem 5: Real World Spam Classification}{3062}{subsubsection.1.3.2}}
\newlabel{problem-5-real-world-spam-classification}{{1.3.2}{3062}{Problem 5: Real World Spam Classification}{subsubsection.1.3.2}{}}
